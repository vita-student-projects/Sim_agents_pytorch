{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!conda install -c conda-forge openexr-python -y\\n!pip install waymo-open-dataset-tf-2-11-0==1.5.2  --no-cache-dir \\n!pip install --upgrade google-cloud-storage'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"!conda install -c conda-forge openexr-python -y\n",
        "!pip install waymo-open-dataset-tf-2-11-0==1.5.2  --no-cache-dir \n",
        "!pip install --upgrade google-cloud-storage\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_G1K_QmTl5vQ",
        "outputId": "198789a1-d9e4-47d3-e463-ca4fcfb7a641"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-31 21:34:36.638578: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-31 21:34:40.966312: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-05-31 21:34:40.966351: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-05-31 21:34:53.876324: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-05-31 21:34:53.876627: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-05-31 21:34:53.876647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch \n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import pickle\n",
        "\n",
        "from waymo_open_dataset.wdl_limited.sim_agents_metrics import metric_features\n",
        "from waymo_open_dataset.wdl_limited.sim_agents_metrics import metrics\n",
        "\n",
        "from waymo_open_dataset.protos import scenario_pb2\n",
        "from waymo_open_dataset.protos import sim_agents_metrics_pb2\n",
        "from waymo_open_dataset.protos import sim_agents_submission_pb2\n",
        "from google.protobuf import text_format\n",
        "\n",
        "from waymo_open_dataset.utils.sim_agents import submission_specs\n",
        "from waymo_open_dataset.utils.sim_agents import test_utils as sim_agents_test_utils\n",
        "from waymo_open_dataset.utils.sim_agents import visualizations\n",
        "from waymo_open_dataset.utils import trajectory_utils\n",
        "\n",
        "# Set matplotlib to jshtml so animations work with colab.\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "# deactivating the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from download import download_from_gcs\n",
        "from prepare_dataset import Prepare_train_dataset, Prepare_validation_dataset, Prepare_test_dataset\n",
        "from data_preprocess import process_waymo_data_with_scenario_proto\n",
        "from Mydataloader import create_dataloader\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x_oklHSal5vR"
      },
      "source": [
        "# Downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9ObwY0Rl5vS",
        "outputId": "5dadc54c-23a1-40b4-f214-34c3d1338676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation.tfrecord-00000-of-00150 already exists in waymo_open_dataset_/validation/validation.tfrecord-00000-of-00150\n",
            "training.tfrecord-00000-of-01000 already exists in waymo_open_dataset_/training/training.tfrecord-00000-of-01000\n",
            "testing.tfrecord-00000-of-00150 already exists in waymo_open_dataset_/testing/testing.tfrecord-00000-of-00150\n"
          ]
        }
      ],
      "source": [
        "# Download samples\n",
        "download_from_gcs('uncompressed/scenario/validation/validation.tfrecord-00000-of-00150')\n",
        "download_from_gcs('uncompressed/scenario/training/training.tfrecord-00000-of-01000')\n",
        "download_from_gcs('uncompressed/scenario/testing/testing.tfrecord-00000-of-00150')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qMTFcOA0l5vT"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-28 12:16:55.288877: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-28 12:16:55.588421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-05-28 12:16:55.588462: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-05-28 12:16:59.106980: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-05-28 12:16:59.107093: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-05-28 12:16:59.107114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "  0%|                                                     | 0/1 [00:00<?, ?it/s]2023-05-28 12:17:03.268069: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2023-05-28 12:17:03.268146: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2023-05-28 12:17:03.268197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-a6b224): /proc/driver/nvidia/version does not exist\n",
            "2023-05-28 12:17:03.380180: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "100%|█████████████████████████████████████████████| 1/1 [01:11<00:00, 71.36s/it]\n",
            "----------------Waymo info train file is saved to scenarios/processed_scenarios_training_infos.pkl----------------\n",
            "  0%|                                                     | 0/1 [00:00<?, ?it/s]2023-05-28 12:18:14.852295: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2023-05-28 12:18:14.852377: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2023-05-28 12:18:14.852424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-a6b224): /proc/driver/nvidia/version does not exist\n",
            "2023-05-28 12:18:14.853355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:39<00:00, 39.84s/it]\n",
            "----------------Waymo info val file is saved to scenarios/processed_scenarios_val_infos.pkl----------------\n"
          ]
        }
      ],
      "source": [
        "!python data_preprocess.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trajectory data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the original trajectories is:  (105, 91, 10)\n",
            "The shape of the tracks to predict until current time step is:  (4, 11, 10)\n",
            "The shape of the tracks to predict after current time step is:  (4, 80, 10)\n",
            "The shape of the track of adv until current time step is:  (11, 10)\n",
            "The shape of the track of adv after current time step is:  (80, 10)\n",
            "The shape of the track of other agents until current time step is:  (104, 11, 10)\n",
            "The shape of the track of other agents after current time step is:  (104, 80, 10)\n"
          ]
        }
      ],
      "source": [
        "# load scenarios/processed_scenarios_training/sample_1a8cc570d620bd31.pkl\n",
        "pickle_file = 'scenarios/processed_scenarios_training/sample_1a8cc570d620bd31.pkl'\n",
        "processed = pickle.load(open(pickle_file, 'rb'))\n",
        "\n",
        "print('The shape of the original trajectories is: ', processed['track_infos']['trajs'].shape)\n",
        "print('The shape of the tracks to predict until current time step is: ', processed['track_infos']['tracks_to_predict_until_current'].shape)\n",
        "print('The shape of the tracks to predict after current time step is: ', processed['track_infos']['tracks_to_predict_future'].shape)\n",
        "print('The shape of the track of adv until current time step is: ', processed['track_infos']['track_of_adv_until_current'].shape)\n",
        "print('The shape of the track of adv after current time step is: ', processed['track_infos']['track_of_adv_future'].shape)\n",
        "print('The shape of the track of other agents until current time step is: ', processed['track_infos']['tracks_of_other_agents_until_current'].shape)\n",
        "print('The shape of the track of other agents after current time step is: ', processed['track_infos']['tracks_of_other_agents_future'].shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Map data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The (shape, type) of the map info lane is:  ((751,), <class 'list'>)\n",
            "The map info inside each lane element are:  dict_keys(['id', 'speed_limit_mph', 'type', 'interpolating', 'entry_lanes', 'exit_lanes', 'left_boundary', 'right_boundary', 'polyline_index'])\n",
            "The (shape, type) of the map info road_line is:  ((77,), <class 'list'>)\n",
            "The map info inside each road_line element are:  dict_keys(['id', 'type', 'polyline_index'])\n",
            "The (shape, type) of the map info road_edge is:  ((79,), <class 'list'>)\n",
            "The map info inside each road_edge element are:  dict_keys(['id', 'type', 'polyline_index'])\n",
            "The (shape, type) of the map info stop_sign is:  ((1,), <class 'list'>)\n",
            "The map info inside each stop_sign element are:  dict_keys(['id', 'lane_ids', 'position', 'polyline_index'])\n",
            "The (shape, type) of the map info crosswalk is:  ((12,), <class 'list'>)\n",
            "The map info inside each crosswalk element are:  dict_keys(['id', 'polyline_index'])\n",
            "The (shape, type) of the map info speed_bump is:  ((18,), <class 'list'>)\n",
            "The map info inside each speed_bump element are:  dict_keys(['id', 'polyline_index'])\n",
            "The (shape, type) of the map info unknown is:  ((19,), <class 'list'>)\n",
            "The map info inside each unknown element are:  dict_keys(['id', 'polyline_index'])\n",
            "The (shape, type) of the map info all_polylines is:  ((45047, 7), <class 'numpy.ndarray'>)\n"
          ]
        }
      ],
      "source": [
        "for key in processed['map_infos'].keys():\n",
        "    print('The (shape, type) of the map info {} is: '.format(key), (np.array(processed['map_infos'][key]).shape, type(processed['map_infos'][key])))\n",
        "    if key != 'all_polylines':\n",
        "        print('The map info inside each {} element are: '.format(key), processed['map_infos'][key][0].keys())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------- batch 0 -----------------\n",
            "----------------- item 0 -----------------\n",
            "The shape of the original trajectories is: torch.Size([80, 91, 10])\n",
            "The shape of the tracks to predict until current time step is: torch.Size([7, 11, 10])\n",
            "The shape of the tracks to predict after current time step is: torch.Size([7, 80, 10])\n",
            "The shape of the track of adv until current time step is: torch.Size([11, 10])\n",
            "The shape of the track of adv after current time step is: torch.Size([80, 10])\n",
            "The shape of the track of other agents until current time step is: torch.Size([79, 11, 10])\n",
            "The shape of the track of other agents after current time step is: torch.Size([79, 80, 10])\n",
            "\n",
            "----------------- batch 1 -----------------\n",
            "----------------- item 0 -----------------\n",
            "The shape of the original trajectories is: torch.Size([38, 91, 10])\n",
            "The shape of the tracks to predict until current time step is: torch.Size([7, 11, 10])\n",
            "The shape of the tracks to predict after current time step is: torch.Size([7, 80, 10])\n",
            "The shape of the track of adv until current time step is: torch.Size([11, 10])\n",
            "The shape of the track of adv after current time step is: torch.Size([80, 10])\n",
            "The shape of the track of other agents until current time step is: torch.Size([37, 11, 10])\n",
            "The shape of the track of other agents after current time step is: torch.Size([37, 80, 10])\n",
            "\n",
            "----------------- batch 2 -----------------\n",
            "----------------- item 0 -----------------\n",
            "The shape of the original trajectories is: torch.Size([19, 91, 10])\n",
            "The shape of the tracks to predict until current time step is: torch.Size([1, 11, 10])\n",
            "The shape of the tracks to predict after current time step is: torch.Size([1, 80, 10])\n",
            "The shape of the track of adv until current time step is: torch.Size([11, 10])\n",
            "The shape of the track of adv after current time step is: torch.Size([80, 10])\n",
            "The shape of the track of other agents until current time step is: torch.Size([18, 11, 10])\n",
            "The shape of the track of other agents after current time step is: torch.Size([18, 80, 10])\n",
            "\n",
            "----------------- batch 3 -----------------\n",
            "----------------- item 0 -----------------\n",
            "The shape of the original trajectories is: torch.Size([77, 91, 10])\n",
            "The shape of the tracks to predict until current time step is: torch.Size([8, 11, 10])\n",
            "The shape of the tracks to predict after current time step is: torch.Size([8, 80, 10])\n",
            "The shape of the track of adv until current time step is: torch.Size([11, 10])\n",
            "The shape of the track of adv after current time step is: torch.Size([80, 10])\n",
            "The shape of the track of other agents until current time step is: torch.Size([76, 11, 10])\n",
            "The shape of the track of other agents after current time step is: torch.Size([76, 80, 10])\n",
            "\n",
            "----------------- batch 4 -----------------\n",
            "----------------- item 0 -----------------\n",
            "The shape of the original trajectories is: torch.Size([74, 91, 10])\n",
            "The shape of the tracks to predict until current time step is: torch.Size([3, 11, 10])\n",
            "The shape of the tracks to predict after current time step is: torch.Size([3, 80, 10])\n",
            "The shape of the track of adv until current time step is: torch.Size([11, 10])\n",
            "The shape of the track of adv after current time step is: torch.Size([80, 10])\n",
            "The shape of the track of other agents until current time step is: torch.Size([73, 11, 10])\n",
            "The shape of the track of other agents after current time step is: torch.Size([73, 80, 10])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "train_directory = 'scenarios/processed_scenarios_training'\n",
        "validation_directory = 'scenarios/processed_scenarios_validation'\n",
        "batch_size = 1\n",
        "train_dataloader = create_dataloader(train_directory, batch_size)\n",
        "validation_dataloader = create_dataloader(validation_directory, batch_size)\n",
        "\n",
        "\n",
        "# Iterate over the dataloader\n",
        "for i, batch in enumerate(train_dataloader):\n",
        "    print('----------------- batch {} -----------------'.format(i))\n",
        "    for j, item in enumerate(batch):\n",
        "        trajectories, tracks_until_current, tracks_future, adv_until_current, adv_future, other_agents_until_current, other_agents_future = item\n",
        "        print('----------------- item {} -----------------'.format(j))\n",
        "        print('The shape of the original trajectories is:', trajectories.shape)\n",
        "        print('The shape of the tracks to predict until current time step is:', tracks_until_current.shape)\n",
        "        print('The shape of the tracks to predict after current time step is:', tracks_future.shape)\n",
        "        print('The shape of the track of adv until current time step is:', adv_until_current.shape)\n",
        "        print('The shape of the track of adv after current time step is:', adv_future.shape)\n",
        "        print('The shape of the track of other agents until current time step is:', other_agents_until_current.shape)\n",
        "        print('The shape of the track of other agents after current time step is:', other_agents_future.shape)\n",
        "        print()\n",
        "    if i > 3:\n",
        "        break  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_with_extrapolation_pytorch(tracks_until_current, print_verbose_comments=False,num_rollouts=submission_specs.N_ROLLOUTS):\n",
        "    \"\"\"Simulate the scenario with extrapolation using PyTorch.\n",
        "\n",
        "    Args:\n",
        "        tracks_until_current: A numpy array of shape [num_objects, num_steps, 10] representing the tracks until current time step.\n",
        "        num_rollouts: An integer representing the number of rollouts to simulate.\n",
        "\n",
        "    Returns:\n",
        "        A numpy array of shape [num_rollouts, num_objects, num_steps, 4] representing the simulated tracks.\n",
        "    \"\"\"\n",
        "    vprint = print if print_verbose_comments else lambda arg: None\n",
        "    # We can verify that all of these objects are valid at the last step.\n",
        "    number_of_valid_objects = int(torch.sum(tracks_until_current[:, -1, -1]))\n",
        "    vprint(f'The number of valid objects at the last step is:{number_of_valid_objects}')\n",
        "    total_number_of_objects = tracks_until_current.shape[0]\n",
        "    vprint(f'Out of:{total_number_of_objects} total number of objects')\n",
        "    states = torch.stack([tracks_until_current[:, :, 0], tracks_until_current[:, :, 1],\n",
        "                         tracks_until_current[:, :, 2], tracks_until_current[:, :, 6]], dim=-1)\n",
        "\n",
        "    num_objects, num_steps, _ = states.shape\n",
        "    last_velocities = states[:, -1, 3] - states[:, -2, 3]\n",
        "    last_velocities = states[:, -1, :3] - states[:, -2, :3]\n",
        "    # We also make the heading constant, so concatenate 0. as angular speed.\n",
        "    last_velocities = torch.cat(\n",
        "        [last_velocities, torch.zeros((num_objects, 1))], dim=-1)\n",
        "    # It can happen that the second to last state of these sim agents might be\n",
        "    # invalid, so we will set a zero speed for them.\n",
        "    vprint(f'Is any 2nd to last state invalid: {torch.logical_not(tracks_until_current[:, -1, -1]).any()}')\n",
        "    vprint(f'This will result in either min or max speed to be really large: {torch.max(torch.abs(last_velocities))}')\n",
        "    valid_diff = torch.logical_and(tracks_until_current[:, -1, -1],\n",
        "                                tracks_until_current[:, -2, -1])\n",
        "    # `last_velocities` shape: (n_objects, 4).\n",
        "    last_velocities = torch.where(valid_diff[:, None],\n",
        "                                last_velocities,\n",
        "                                torch.zeros_like(last_velocities))\n",
        "    vprint(f'Now this should be back to a normal value: {torch.max(torch.abs(last_velocities))}')\n",
        "\n",
        "    # Now we carry over a simulation. As we discussed, we actually want 32 parallel\n",
        "    # simulations, so we make this batched from the very beginning. We add some\n",
        "    # random noise on top of our actions to make sure the behaviors are different.\n",
        "    # To properly scale the noise, we get the max velocities (average over all\n",
        "    # objects, corresponding to axis 0) in each of the dimensions (x/y/z/heading).\n",
        "    NOISE_SCALE = 0.01\n",
        "    # `max_action` shape: (4,).\n",
        "    max_action = torch.max(last_velocities, dim=0)[0]\n",
        "    # We create `simulated_states` with shape (n_rollouts, n_objects, n_steps, 4).\n",
        "    simulated_states = states.unsqueeze(0).repeat(num_rollouts, 1, 1, 1)\n",
        "    simulated_states = simulated_states[:, :, -1, :].unsqueeze(2)\n",
        "    vprint(f'Shape of simulated_states: {simulated_states.shape}')\n",
        "\n",
        "    for step in range(submission_specs.N_SIMULATION_STEPS):\n",
        "        current_state = simulated_states[:, :, -1, :]\n",
        "        # Random actions, take a normal and normalize by min/max actions\n",
        "        action_noise = torch.randn_like(current_state) * NOISE_SCALE\n",
        "        actions_with_noise = last_velocities.unsqueeze(0) + (action_noise * max_action)\n",
        "        next_state = current_state + actions_with_noise\n",
        "        simulated_states = torch.cat(\n",
        "            [simulated_states, next_state[:, :, None, :]], dim=2)\n",
        "        \n",
        "    # now remove first state, as it is the same as the last state of the previous\n",
        "    simulated_states = simulated_states[:, :, 1:, :]\n",
        "    vprint(f'Final Shape of simulated_states: {simulated_states.shape}')\n",
        "    \n",
        "    return simulated_states.numpy()\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of valid objects at the last step is:3\n",
            "Out of:3 total number of objects\n",
            "Is any 2nd to last state invalid: False\n",
            "This will result in either min or max speed to be really large: 0.5867919921875\n",
            "Now this should be back to a normal value: 0.5867919921875\n",
            "Shape of simulated_states: torch.Size([32, 3, 1, 4])\n",
            "Final Shape of simulated_states: torch.Size([32, 3, 80, 4])\n"
          ]
        }
      ],
      "source": [
        "simulated_tracks_sample = simulate_with_extrapolation_pytorch(tracks_until_current, print_verbose_comments=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simulate with extrapolation for all batches\n",
        "\n",
        "def simulate_with_extrapolation_pytorch_all_batches(dataloader):\n",
        "    \"\"\"Simulate the scenario with extrapolation using PyTorch.\n",
        "\n",
        "    Args:\n",
        "        dataloader: A dataloader object.\n",
        "\n",
        "    Returns:\n",
        "        A numpy array of shape [num_rollouts, num_objects, num_steps, 4] representing the simulated tracks.\n",
        "    \"\"\"\n",
        "    simulated_tracks = []\n",
        "    for  batch in dataloader:\n",
        "        for j, item in enumerate(batch):\n",
        "            trajectories, tracks_until_current, tracks_future, adv_until_current, adv_future, other_agents_until_current, other_agents_future = item\n",
        "            simulated_tracks.append(simulate_with_extrapolation_pytorch(tracks_until_current))\n",
        "    return simulated_tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "simulated_tracks = simulate_with_extrapolation_pytorch_all_batches(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
